{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_datasets(X):\n",
    "    X_new = []\n",
    "    for i in range(X.shape[0]):\n",
    "        if(i % 100 == 0):\n",
    "            print(i)\n",
    "        img = scipy.ndimage.interpolation.zoom(X[i], (8.0, 8.0, 1.0))\n",
    "        X_new.append(img)\n",
    "    X_new = np.asarray(X_new, dtype=np.float32)\n",
    "    return X_new.reshape([X.shape[0], 256, 256, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resizing Validation...')\n",
    "X_val = resize_datasets(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resizing Test...')\n",
    "X_test = resize_datasets(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Resizing Training...')\n",
    "X_train = resize_datasets(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"train_dataset.pickle\",\"wb\")\n",
    "pickle.dump(X_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"validation_dataset.pickle\",\"rb\")\n",
    "X_val = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_in = open(\"dataset.pickle\",\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "y_train = pickle.load(pickle_in)\n",
    "X_test = pickle.load(pickle_in)\n",
    "y_test = pickle.load(pickle_in)\n",
    "X_val = pickle.load(pickle_in)\n",
    "y_val = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:30] = X_train[:30]\n",
    "y_test[:30] = y_train[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_size = 200\n",
    "test_set_size = 100\n",
    "valid_set_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    - TensorFlow Tensor of shape (N, D1, ..., DM)\n",
    "    \n",
    "    Output:\n",
    "    - TensorFlow Tensor of shape (N, D1 * ... * DM)\n",
    "    \"\"\"\n",
    "    N = tf.shape(x)[0]\n",
    "    return tf.reshape(x, (N, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.get_variable(name = 'input', shape=[1,256,256,3], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#globals used in the assignment\n",
    "IMG_SHAPE = [256, 256, 3] # height, width, channels\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# training hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "DISPLAY_STEP = 1\n",
    "VALIDATION_STEP = 1000\n",
    "SAVE_STEP = 1000\n",
    "CKPT_PATH = './ckpt'\n",
    "SUMMARY_PATH = './summary'\n",
    "\n",
    "# net architecture hyperparamaters\n",
    "LAMBDA = 5e-4 #for weight decay\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# test hyper parameters\n",
    "K_PATCHES = 5\n",
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(tfe.Network):\n",
    "    def __init__(self, training):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.training = training\n",
    "        \n",
    "        # convolutional layers\n",
    "        conv_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        \n",
    "        self.conv1 = self.track_layer(\n",
    "            tf.layers.Conv2D(\n",
    "                96,\n",
    "                11,\n",
    "                4,\n",
    "                'SAME',\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=conv_init))\n",
    "        self.pool1 = self.track_layer(tf.layers.MaxPooling2D(3, 2, 'VALID'))\n",
    "\n",
    "        self.conv2 = self.track_layer(\n",
    "            tf.layers.Conv2D(\n",
    "                256,\n",
    "                5,\n",
    "                1,\n",
    "                'SAME',\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=conv_init))\n",
    "        self.pool2 = self.track_layer(tf.layers.MaxPooling2D(3, 2, 'VALID'))\n",
    "\n",
    "        self.conv3 = self.track_layer(\n",
    "            tf.layers.Conv2D(\n",
    "                384,\n",
    "                3,\n",
    "                1,\n",
    "                'SAME',\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=conv_init))\n",
    "\n",
    "        self.conv4 = self.track_layer(\n",
    "            tf.layers.Conv2D(\n",
    "                384,\n",
    "                3,\n",
    "                1,\n",
    "                'SAME',\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=conv_init))\n",
    "\n",
    "        self.conv5 = self.track_layer(\n",
    "            tf.layers.Conv2D(\n",
    "                256,\n",
    "                3,\n",
    "                1,\n",
    "                'SAME',\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=conv_init))\n",
    "        self.pool5 = self.track_layer(tf.layers.MaxPooling2D(3, 2, 'VALID'))\n",
    "\n",
    "        # fully connected layers\n",
    "\n",
    "        fc_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        self.fc1 = self.track_layer(\n",
    "            tf.layers.Dense(\n",
    "                512, kernel_initializer=fc_init))\n",
    "        self.drop1 = self.track_layer(tf.layers.Dropout(DROPOUT))\n",
    "        \n",
    "        #attention estimators\n",
    "        self.atten1_fc = self.track_layer(\n",
    "            tf.layers.Dense(\n",
    "                512, kernel_initializer=fc_init))\n",
    "        \n",
    "        self.atten2_fc = self.track_layer(\n",
    "            tf.layers.Dense(\n",
    "                512, kernel_initializer=fc_init))\n",
    "        \n",
    "        self.out = self.track_layer(\n",
    "            tf.layers.Dense(NUM_CLASSES, kernel_initializer=fc_init))\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\" Function that executes the model \"\"\"\n",
    "        output = self.conv1(x)\n",
    "        #print(\"After conv1 : \" + str(output.numpy().shape))\n",
    "        output = tf.nn.lrn(\n",
    "            output, depth_radius=2, bias=1.0, alpha=2e-05, beta=0.75)\n",
    "        #print(\"After normalisation1 : \" + str(output.numpy().shape))\n",
    "        output = self.pool1(output)\n",
    "        #print(\"After pool1 : \" + str(output.numpy().shape))\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        #print(\"After conv2 : \" + str(output.numpy().shape))\n",
    "        output = tf.nn.lrn(\n",
    "            output, depth_radius=2, bias=1.0, alpha=2e-05, beta=0.75)\n",
    "        #print(\"After normalisation2 : \" + str(output.numpy().shape))\n",
    "        output = self.pool2(output)\n",
    "        #print(\"After pool2 : \" + str(output.numpy().shape))\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        #print(\"After conv3 : \" + str(output.numpy().shape))\n",
    "\n",
    "        output = self.conv4(output)\n",
    "        #print(\"After conv4 : \" + str(output.numpy().shape))\n",
    "        conv4_output = output\n",
    "\n",
    "        output = self.conv5(output)\n",
    "        #print(\"After conv5 : \" + str(output.numpy().shape))\n",
    "        conv5_output = output\n",
    "        \n",
    "        output = self.pool5(output)\n",
    "        #print(\"After pool5 : \" + str(output.numpy().shape))\n",
    "\n",
    "        output = tf.layers.flatten(output)\n",
    "        #print(\"After flattening : \" + str(output.numpy().shape))\n",
    "\n",
    "        output = self.fc1(output)\n",
    "        #print(\"After fc1 : \" + str(output.numpy().shape))\n",
    "        if self.training:\n",
    "            output = self.drop1(output)\n",
    "            #print(\"After drop1 : \" + str(output.numpy().shape))\n",
    "        \n",
    "        #logic for attention estimators\n",
    "        g = tf.reshape(output, (output.shape[0], output.shape[1], 1))\n",
    "        #print(\"G dimensions : \" + str(g.numpy().shape))\n",
    "        \n",
    "#         print(\"\\nATTENTION 1 :-\")\n",
    "        atten1_flattened_output = tf.reshape(conv4_output, (conv4_output.shape[0], conv4_output.shape[1] * conv4_output.shape[2], conv4_output.shape[3]))\n",
    "#         print(\"After Flattening : \" + str(atten1_flattened_output.numpy().shape))\n",
    "        atten1_output = self.atten1_fc(atten1_flattened_output)\n",
    "#         print(\"After FC : \" + str(atten1_output.numpy().shape))\n",
    "        atten1_output = tf.matmul(atten1_output, g)\n",
    "#         print(\"After Dot Product with G : \" + str(atten1_output.numpy().shape))\n",
    "        atten1_output = tf.reshape(atten1_output, (atten1_output.shape[0], atten1_output.shape[2], atten1_output.shape[1]))\n",
    "        attention1_values = atten1_output\n",
    "        atten1_output = tf.nn.softmax(atten1_output)\n",
    "#         print(\"After Reshaping and applying Softmax : \" + str(atten1_output.numpy().shape))\n",
    "        atten1_output = tf.matmul(atten1_output, atten1_flattened_output)\n",
    "#         print(\"G_a1 Dimensions : \" + str(atten1_output.numpy().shape))\n",
    "        \n",
    "        #print(\"\\nATTENTION 2 :-\")\n",
    "        atten2_flattened_output = tf.reshape(conv5_output, (conv5_output.shape[0], conv5_output.shape[1] * conv5_output.shape[2], conv5_output.shape[3]))\n",
    "        #print(\"After Flattening : \" + str(atten2_flattened_output.numpy().shape))\n",
    "        atten2_output = self.atten2_fc(atten2_flattened_output)\n",
    "        #print(\"After FC : \" + str(atten2_output.numpy().shape))\n",
    "        atten2_output = tf.matmul(atten2_output, g)\n",
    "        #print(\"After Dot Product with G : \" + str(atten2_output.numpy().shape))\n",
    "        atten2_output = tf.reshape(atten2_output, (atten2_output.shape[0], atten2_output.shape[2], atten2_output.shape[1]))\n",
    "        attention2_values = atten2_output\n",
    "        atten2_output = tf.nn.softmax(atten2_output)\n",
    "        #print(\"After Reshaping and applying Softmax : \" + str(atten2_output.numpy().shape))\n",
    "        atten2_output = tf.matmul(atten2_output, atten2_flattened_output)\n",
    "        #print(\"G_a1 Dimensions : \" + str(atten2_output.numpy().shape))\n",
    "        \n",
    "        #print(\"\\nCOMBINING BY OPTION 1 :-\")\n",
    "        output = tf.concat([atten1_output, atten2_output], 2)\n",
    "        output = tf.reshape(output, (output.shape[0], output.shape[2]))\n",
    "        #print(\"Option 1 Dimensions : \" + str(output.numpy().shape))\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output,attention1_values,attention2_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(net, mode, x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss for a given batch of examples\n",
    "    Args:\n",
    "        mode, string 'train' or 'val'\n",
    "        x, tf tensor representing a batch of images\n",
    "        y, tf tensor representing a batch of labels\n",
    "    Returns:\n",
    "        the loss between the predictions on the images and the groundtruths\n",
    "    \"\"\"\n",
    "    pred = net(x)[0]\n",
    "    y = tf.one_hot(y, NUM_CLASSES)\n",
    "\n",
    "    loss_value = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=pred)\n",
    "    weight_decay = tf.reduce_sum(LAMBDA * tf.stack([tf.nn.l2_loss(v) for v in net.variables]))\n",
    "\n",
    "    total_loss = loss_value + weight_decay\n",
    "\n",
    "    tf.contrib.summary.scalar(mode + '/loss', total_loss)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net, mode, x, y):\n",
    "    \"\"\"\n",
    "    Computes the accuracy for a given batch of examples\n",
    "    Args:\n",
    "        mode, string 'train' or 'val'\n",
    "        x, tf tensor representing a batch of images\n",
    "        y, tf tensor representing a batch of labels\n",
    "    Returns:\n",
    "        the accuracy of the predictions on the images and the groundtruths\n",
    "    \"\"\"\n",
    "    pred = tf.nn.softmax(net(x)[0])\n",
    "    y = tf.one_hot(y, NUM_CLASSES)\n",
    "\n",
    "    accuracy_value = tf.reduce_sum(\n",
    "                tf.cast(\n",
    "                    tf.equal(\n",
    "                        tf.argmax(pred, axis=1, output_type=tf.int64),\n",
    "                        tf.argmax(y, axis=1, output_type=tf.int64)\n",
    "                    ),\n",
    "                    dtype=tf.float32\n",
    "                ) \n",
    "            ) / float(pred.shape[0].value)\n",
    "\n",
    "    tf.contrib.summary.scalar(mode +'/accuracy', accuracy_value)\n",
    "\n",
    "    return accuracy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = Dataset(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=valid_set_size, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (16, 256, 256, 3) (16,)\n",
      "1 (16, 256, 256, 3) (16,)\n",
      "2 (16, 256, 256, 3) (16,)\n",
      "3 (16, 256, 256, 3) (16,)\n",
      "4 (16, 256, 256, 3) (16,)\n",
      "5 (16, 256, 256, 3) (16,)\n",
      "6 (16, 256, 256, 3) (16,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "#     print(y)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for printing the log with the timestamp\n",
    "logging = tf.logging\n",
    "logging.set_verbosity(logging.INFO)\n",
    "def log_msg(msg):\n",
    "       logging.info(f'{time.ctime()}: {msg}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_STEP = 1\n",
    "VALIDATION_STEP = 5\n",
    "SAVE_STEP = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(net):\n",
    "    \"\"\"\n",
    "    Training procedure\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    step_time = 0.0\n",
    "    \n",
    "    writer = tf.contrib.summary.create_summary_file_writer(SUMMARY_PATH)\n",
    "#     optimizer = tf.train.MomentumOptimizer(learning_rate=LEARNING_RATE, momentum=MOMENTUM)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    epoch = tfe.Variable(0, name='epoch', dtype=tf.float32, trainable=False)\n",
    "    all_variables = (net.variables + optimizer.variables() + [global_step] + [epoch])\n",
    "    \n",
    "    with writer.as_default():\n",
    "        with tf.contrib.summary.record_summaries_every_n_global_steps(DISPLAY_STEP):\n",
    "            for e in range(int(epoch.numpy()), EPOCHS):\n",
    "                tf.assign(epoch, e)\n",
    "                for (batch_i, (images, labels)) in enumerate(train_dset):\n",
    "                    global_step = tf.train.get_global_step()\n",
    "                    step = global_step.numpy() + 1\n",
    "                    images = tf.to_float(images)\n",
    "                    step_start_time = int(round(time.time() * 1000))\n",
    "                    \n",
    "                    optimizer.minimize(lambda: loss(net, 'train', images, labels), global_step=global_step)\n",
    "\n",
    "                    step_end_time = int(round(time.time() * 1000))\n",
    "                    step_time += step_end_time - step_start_time\n",
    "\n",
    "                    if (step % DISPLAY_STEP) == 0:\n",
    "                        l = loss(net, 'train', images, labels)\n",
    "                        a = accuracy(net, 'train', images, labels).numpy()\n",
    "                        log_msg('Epoch: {:03d} Step/Batch: {:09d} Step mean time: {:04d}ms \\nLoss: {:.7f} Training accuracy: {:.4f}'.format(e, step, int(step_time / step), l, a))\n",
    "\n",
    "                    if (step % VALIDATION_STEP) == 0:\n",
    "                        for (_, (val_images, val_labels)) in enumerate(val_dset):\n",
    "#                             val_images, val_labels = tfe.Iterator(val_dset).next()\n",
    "                            l = loss(net, 'val', val_images, val_labels)\n",
    "                            a = accuracy(net, 'val', val_images, val_labels).numpy()\n",
    "                            int_time = time.time() - start_time\n",
    "                            log_msg('Elapsed time: {:04d}ms --- Loss: {:.7f} Validation accuracy: {:.4f}'.format(int(int_time), l, a))\n",
    "\n",
    "                    if (step % SAVE_STEP) == 0:\n",
    "                        tfe.Saver(net.variables).save(os.path.join(CKPT_PATH, 'net.ckpt'), global_step=global_step)\n",
    "                        log_msg('Variables saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:** tfe.Network is deprecated and will be removed in a future version.\n",
      "\n",
      "Please inherit from `tf.keras.Model`, and see its documentation for details. `tf.keras.Model` should be a drop-in replacement for `tfe.Network` in most cases, but note that `track_layer` is no longer necessary or supported. Instead, `Layer` instances are tracked on attribute assignment (see the section of `tf.keras.Model`'s documentation on subclassing). Since the output of `track_layer` is often assigned to an attribute anyway, most code can be ported by simply removing the `track_layer` calls.\n",
      "\n",
      "`tf.keras.Model` works with all TensorFlow `Layer` instances, including those from `tf.layers`, but switching to the `tf.keras.layers` versions along with the migration to `tf.keras.Model` is recommended, since it will preserve variable names. Feel free to import it with an alias to avoid excess typing :).\n"
     ]
    }
   ],
   "source": [
    "alexnet = AlexNet(training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation Warning: create_summary_file_writer was renamed to create_file_writer\n",
      "INFO:tensorflow:Sat Nov 17 20:53:38 2018: Epoch: 000 Step/Batch: 000000262 Step mean time: 0017ms \n",
      "Loss: 2.8710556 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 20:53:45 2018: Epoch: 000 Step/Batch: 000000263 Step mean time: 0032ms \n",
      "Loss: 3.0132735 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 20:53:52 2018: Epoch: 000 Step/Batch: 000000264 Step mean time: 0049ms \n",
      "Loss: 3.0348649 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:53:58 2018: Epoch: 000 Step/Batch: 000000265 Step mean time: 0064ms \n",
      "Loss: 3.0505106 Training accuracy: 0.0000\n",
      "INFO:tensorflow:Sat Nov 17 20:54:06 2018: Elapsed time: 0034ms --- Loss: 3.0264702 Validation accuracy: 0.1000\n",
      "INFO:tensorflow:Sat Nov 17 20:54:13 2018: Epoch: 000 Step/Batch: 000000266 Step mean time: 0080ms \n",
      "Loss: 3.0174997 Training accuracy: 0.0625\n",
      "INFO:tensorflow:Sat Nov 17 20:54:19 2018: Epoch: 000 Step/Batch: 000000267 Step mean time: 0095ms \n",
      "Loss: 3.0163426 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:54:26 2018: Epoch: 000 Step/Batch: 000000268 Step mean time: 0110ms \n",
      "Loss: 2.9513950 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 20:54:32 2018: Epoch: 000 Step/Batch: 000000269 Step mean time: 0125ms \n",
      "Loss: 3.0261927 Training accuracy: 0.0625\n",
      "INFO:tensorflow:Sat Nov 17 20:54:39 2018: Epoch: 000 Step/Batch: 000000270 Step mean time: 0139ms \n",
      "Loss: 2.9968386 Training accuracy: 0.0000\n",
      "INFO:tensorflow:Sat Nov 17 20:54:47 2018: Elapsed time: 0075ms --- Loss: 2.9941361 Validation accuracy: 0.1200\n",
      "INFO:tensorflow:Sat Nov 17 20:54:47 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 20:54:54 2018: Epoch: 000 Step/Batch: 000000271 Step mean time: 0153ms \n",
      "Loss: 3.0007095 Training accuracy: 0.0000\n",
      "INFO:tensorflow:Sat Nov 17 20:55:01 2018: Epoch: 000 Step/Batch: 000000272 Step mean time: 0167ms \n",
      "Loss: 2.9767051 Training accuracy: 0.0625\n",
      "INFO:tensorflow:Sat Nov 17 20:55:07 2018: Epoch: 000 Step/Batch: 000000273 Step mean time: 0181ms \n",
      "Loss: 3.0042334 Training accuracy: 0.0625\n",
      "INFO:tensorflow:Sat Nov 17 20:55:11 2018: Epoch: 000 Step/Batch: 000000274 Step mean time: 0188ms \n",
      "Loss: 2.9668908 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:55:17 2018: Epoch: 001 Step/Batch: 000000275 Step mean time: 0202ms \n",
      "Loss: 2.9237156 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 20:55:24 2018: Elapsed time: 0113ms --- Loss: 2.9564929 Validation accuracy: 0.1200\n",
      "INFO:tensorflow:Sat Nov 17 20:55:31 2018: Epoch: 001 Step/Batch: 000000276 Step mean time: 0216ms \n",
      "Loss: 2.9444246 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:55:37 2018: Epoch: 001 Step/Batch: 000000277 Step mean time: 0229ms \n",
      "Loss: 2.9595902 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 20:55:47 2018: Epoch: 001 Step/Batch: 000000278 Step mean time: 0250ms \n",
      "Loss: 2.9231665 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:55:57 2018: Epoch: 001 Step/Batch: 000000279 Step mean time: 0273ms \n",
      "Loss: 2.9076924 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:56:07 2018: Epoch: 001 Step/Batch: 000000280 Step mean time: 0293ms \n",
      "Loss: 2.8795218 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 20:56:18 2018: Elapsed time: 0166ms --- Loss: 2.9043789 Validation accuracy: 0.2000\n",
      "INFO:tensorflow:Sat Nov 17 20:56:18 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 20:56:28 2018: Epoch: 001 Step/Batch: 000000281 Step mean time: 0313ms \n",
      "Loss: 2.8872726 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 20:56:37 2018: Epoch: 001 Step/Batch: 000000282 Step mean time: 0332ms \n",
      "Loss: 2.8093920 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:56:47 2018: Epoch: 001 Step/Batch: 000000283 Step mean time: 0352ms \n",
      "Loss: 2.9363847 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:56:56 2018: Epoch: 001 Step/Batch: 000000284 Step mean time: 0373ms \n",
      "Loss: 2.8066390 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 20:57:05 2018: Epoch: 001 Step/Batch: 000000285 Step mean time: 0391ms \n",
      "Loss: 2.7587698 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:57:16 2018: Elapsed time: 0224ms --- Loss: 2.8590100 Validation accuracy: 0.1800\n",
      "INFO:tensorflow:Sat Nov 17 20:57:25 2018: Epoch: 001 Step/Batch: 000000286 Step mean time: 0409ms \n",
      "Loss: 2.8328547 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 20:57:30 2018: Epoch: 001 Step/Batch: 000000287 Step mean time: 0418ms \n",
      "Loss: 2.6234894 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 20:57:39 2018: Epoch: 002 Step/Batch: 000000288 Step mean time: 0436ms \n",
      "Loss: 2.7531362 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 20:57:48 2018: Epoch: 002 Step/Batch: 000000289 Step mean time: 0453ms \n",
      "Loss: 3.0784245 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:57:57 2018: Epoch: 002 Step/Batch: 000000290 Step mean time: 0471ms \n",
      "Loss: 3.1004987 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:58:08 2018: Elapsed time: 0276ms --- Loss: 3.0406289 Validation accuracy: 0.1400\n",
      "INFO:tensorflow:Sat Nov 17 20:58:09 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 20:58:18 2018: Epoch: 002 Step/Batch: 000000291 Step mean time: 0489ms \n",
      "Loss: 2.9880414 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:58:27 2018: Epoch: 002 Step/Batch: 000000292 Step mean time: 0506ms \n",
      "Loss: 2.9198396 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:58:36 2018: Epoch: 002 Step/Batch: 000000293 Step mean time: 0524ms \n",
      "Loss: 2.7187452 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 20:58:45 2018: Epoch: 002 Step/Batch: 000000294 Step mean time: 0542ms \n",
      "Loss: 2.8218141 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 20:58:54 2018: Epoch: 002 Step/Batch: 000000295 Step mean time: 0559ms \n",
      "Loss: 2.7754226 Training accuracy: 0.0625\n",
      "INFO:tensorflow:Sat Nov 17 20:59:05 2018: Elapsed time: 0334ms --- Loss: 2.8516002 Validation accuracy: 0.2200\n",
      "INFO:tensorflow:Sat Nov 17 20:59:15 2018: Epoch: 002 Step/Batch: 000000296 Step mean time: 0577ms \n",
      "Loss: 2.8229065 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 20:59:24 2018: Epoch: 002 Step/Batch: 000000297 Step mean time: 0594ms \n",
      "Loss: 2.7460668 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 20:59:34 2018: Epoch: 002 Step/Batch: 000000298 Step mean time: 0610ms \n",
      "Loss: 2.7540965 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 20:59:44 2018: Epoch: 002 Step/Batch: 000000299 Step mean time: 0629ms \n",
      "Loss: 2.8582280 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 20:59:50 2018: Epoch: 002 Step/Batch: 000000300 Step mean time: 0639ms \n",
      "Loss: 2.7142878 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:00:02 2018: Elapsed time: 0390ms --- Loss: 2.8708234 Validation accuracy: 0.1000\n",
      "INFO:tensorflow:Sat Nov 17 21:00:02 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:00:11 2018: Epoch: 003 Step/Batch: 000000301 Step mean time: 0655ms \n",
      "Loss: 2.8712838 Training accuracy: 0.0625\n",
      "INFO:tensorflow:Sat Nov 17 21:00:20 2018: Epoch: 003 Step/Batch: 000000302 Step mean time: 0672ms \n",
      "Loss: 2.9174576 Training accuracy: 0.0625\n",
      "INFO:tensorflow:Sat Nov 17 21:00:29 2018: Epoch: 003 Step/Batch: 000000303 Step mean time: 0688ms \n",
      "Loss: 2.9648902 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 21:00:38 2018: Epoch: 003 Step/Batch: 000000304 Step mean time: 0704ms \n",
      "Loss: 2.8541586 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:00:47 2018: Epoch: 003 Step/Batch: 000000305 Step mean time: 0720ms \n",
      "Loss: 2.8149929 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 21:00:59 2018: Elapsed time: 0447ms --- Loss: 2.8207884 Validation accuracy: 0.1800\n",
      "INFO:tensorflow:Sat Nov 17 21:01:09 2018: Epoch: 003 Step/Batch: 000000306 Step mean time: 0738ms \n",
      "Loss: 2.7740855 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:01:18 2018: Epoch: 003 Step/Batch: 000000307 Step mean time: 0755ms \n",
      "Loss: 2.8239028 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:01:28 2018: Epoch: 003 Step/Batch: 000000308 Step mean time: 0771ms \n",
      "Loss: 2.7848010 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 21:01:37 2018: Epoch: 003 Step/Batch: 000000309 Step mean time: 0787ms \n",
      "Loss: 2.8123152 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:01:46 2018: Epoch: 003 Step/Batch: 000000310 Step mean time: 0803ms \n",
      "Loss: 2.8019288 Training accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Sat Nov 17 21:01:59 2018: Elapsed time: 0507ms --- Loss: 2.8308661 Validation accuracy: 0.1600\n",
      "INFO:tensorflow:Sat Nov 17 21:02:00 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:02:10 2018: Epoch: 003 Step/Batch: 000000311 Step mean time: 0821ms \n",
      "Loss: 2.8231897 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:02:20 2018: Epoch: 003 Step/Batch: 000000312 Step mean time: 0837ms \n",
      "Loss: 2.8047483 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 21:02:24 2018: Epoch: 003 Step/Batch: 000000313 Step mean time: 0843ms \n",
      "Loss: 2.6855578 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:02:33 2018: Epoch: 004 Step/Batch: 000000314 Step mean time: 0858ms \n",
      "Loss: 2.7170143 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:02:42 2018: Epoch: 004 Step/Batch: 000000315 Step mean time: 0873ms \n",
      "Loss: 2.7710304 Training accuracy: 0.0625\n",
      "INFO:tensorflow:Sat Nov 17 21:02:55 2018: Elapsed time: 0564ms --- Loss: 2.7848897 Validation accuracy: 0.2200\n",
      "INFO:tensorflow:Sat Nov 17 21:03:05 2018: Epoch: 004 Step/Batch: 000000316 Step mean time: 0889ms \n",
      "Loss: 2.8027296 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:03:16 2018: Epoch: 004 Step/Batch: 000000317 Step mean time: 0908ms \n",
      "Loss: 2.7840810 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:03:26 2018: Epoch: 004 Step/Batch: 000000318 Step mean time: 0924ms \n",
      "Loss: 2.7351871 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:03:36 2018: Epoch: 004 Step/Batch: 000000319 Step mean time: 0940ms \n",
      "Loss: 2.6569800 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:03:46 2018: Epoch: 004 Step/Batch: 000000320 Step mean time: 0956ms \n",
      "Loss: 2.7988937 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:03:58 2018: Elapsed time: 0627ms --- Loss: 2.7628915 Validation accuracy: 0.2000\n",
      "INFO:tensorflow:Sat Nov 17 21:03:59 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:04:09 2018: Epoch: 004 Step/Batch: 000000321 Step mean time: 0972ms \n",
      "Loss: 2.5211890 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:04:18 2018: Epoch: 004 Step/Batch: 000000322 Step mean time: 0988ms \n",
      "Loss: 2.7008886 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:04:27 2018: Epoch: 004 Step/Batch: 000000323 Step mean time: 1002ms \n",
      "Loss: 2.6169999 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:04:37 2018: Epoch: 004 Step/Batch: 000000324 Step mean time: 1016ms \n",
      "Loss: 2.4960163 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:04:46 2018: Epoch: 004 Step/Batch: 000000325 Step mean time: 1031ms \n",
      "Loss: 2.6756282 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:04:58 2018: Elapsed time: 0686ms --- Loss: 2.7120037 Validation accuracy: 0.1800\n",
      "INFO:tensorflow:Sat Nov 17 21:05:03 2018: Epoch: 004 Step/Batch: 000000326 Step mean time: 1037ms \n",
      "Loss: 2.3418212 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:05:13 2018: Epoch: 005 Step/Batch: 000000327 Step mean time: 1053ms \n",
      "Loss: 2.6643672 Training accuracy: 0.0625\n",
      "INFO:tensorflow:Sat Nov 17 21:05:23 2018: Epoch: 005 Step/Batch: 000000328 Step mean time: 1068ms \n",
      "Loss: 2.6624069 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:05:32 2018: Epoch: 005 Step/Batch: 000000329 Step mean time: 1084ms \n",
      "Loss: 2.9103315 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:05:42 2018: Epoch: 005 Step/Batch: 000000330 Step mean time: 1098ms \n",
      "Loss: 2.7390034 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:05:53 2018: Elapsed time: 0741ms --- Loss: 2.6275289 Validation accuracy: 0.2200\n",
      "INFO:tensorflow:Sat Nov 17 21:05:54 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:06:03 2018: Epoch: 005 Step/Batch: 000000331 Step mean time: 1113ms \n",
      "Loss: 2.6724923 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:06:13 2018: Epoch: 005 Step/Batch: 000000332 Step mean time: 1127ms \n",
      "Loss: 2.4961066 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:06:22 2018: Epoch: 005 Step/Batch: 000000333 Step mean time: 1141ms \n",
      "Loss: 2.6151509 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:06:31 2018: Epoch: 005 Step/Batch: 000000334 Step mean time: 1154ms \n",
      "Loss: 2.5222104 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:06:41 2018: Epoch: 005 Step/Batch: 000000335 Step mean time: 1168ms \n",
      "Loss: 2.6156814 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 21:06:52 2018: Elapsed time: 0800ms --- Loss: 2.7034006 Validation accuracy: 0.2000\n",
      "INFO:tensorflow:Sat Nov 17 21:07:01 2018: Epoch: 005 Step/Batch: 000000336 Step mean time: 1182ms \n",
      "Loss: 2.5040448 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:07:11 2018: Epoch: 005 Step/Batch: 000000337 Step mean time: 1195ms \n",
      "Loss: 2.5168600 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:07:20 2018: Epoch: 005 Step/Batch: 000000338 Step mean time: 1209ms \n",
      "Loss: 2.5027046 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:07:24 2018: Epoch: 005 Step/Batch: 000000339 Step mean time: 1213ms \n",
      "Loss: 2.3609741 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:07:34 2018: Epoch: 006 Step/Batch: 000000340 Step mean time: 1227ms \n",
      "Loss: 2.3357835 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:07:45 2018: Elapsed time: 0854ms --- Loss: 2.6088028 Validation accuracy: 0.2200\n",
      "INFO:tensorflow:Sat Nov 17 21:07:46 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:07:57 2018: Epoch: 006 Step/Batch: 000000341 Step mean time: 1243ms \n",
      "Loss: 2.6158342 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:08:06 2018: Epoch: 006 Step/Batch: 000000342 Step mean time: 1256ms \n",
      "Loss: 2.6686225 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:08:16 2018: Epoch: 006 Step/Batch: 000000343 Step mean time: 1269ms \n",
      "Loss: 2.5927789 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:08:25 2018: Epoch: 006 Step/Batch: 000000344 Step mean time: 1283ms \n",
      "Loss: 2.6390033 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:08:35 2018: Epoch: 006 Step/Batch: 000000345 Step mean time: 1296ms \n",
      "Loss: 2.3674519 Training accuracy: 0.5625\n",
      "INFO:tensorflow:Sat Nov 17 21:08:47 2018: Elapsed time: 0915ms --- Loss: 2.6215804 Validation accuracy: 0.2400\n",
      "INFO:tensorflow:Sat Nov 17 21:08:57 2018: Epoch: 006 Step/Batch: 000000346 Step mean time: 1310ms \n",
      "Loss: 2.4445264 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:09:06 2018: Epoch: 006 Step/Batch: 000000347 Step mean time: 1322ms \n",
      "Loss: 2.4622290 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 21:09:16 2018: Epoch: 006 Step/Batch: 000000348 Step mean time: 1336ms \n",
      "Loss: 2.6452608 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:09:25 2018: Epoch: 006 Step/Batch: 000000349 Step mean time: 1349ms \n",
      "Loss: 2.4051630 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:09:35 2018: Epoch: 006 Step/Batch: 000000350 Step mean time: 1362ms \n",
      "Loss: 2.4656279 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:09:48 2018: Elapsed time: 0976ms --- Loss: 2.6423647 Validation accuracy: 0.1800\n",
      "INFO:tensorflow:Sat Nov 17 21:09:48 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:09:59 2018: Epoch: 006 Step/Batch: 000000351 Step mean time: 1376ms \n",
      "Loss: 2.6186173 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:10:04 2018: Epoch: 006 Step/Batch: 000000352 Step mean time: 1381ms \n",
      "Loss: 2.2506399 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:10:14 2018: Epoch: 007 Step/Batch: 000000353 Step mean time: 1394ms \n",
      "Loss: 2.3811936 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:10:24 2018: Epoch: 007 Step/Batch: 000000354 Step mean time: 1407ms \n",
      "Loss: 2.6013861 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:10:34 2018: Epoch: 007 Step/Batch: 000000355 Step mean time: 1420ms \n",
      "Loss: 2.6244721 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:10:46 2018: Elapsed time: 1034ms --- Loss: 2.6716857 Validation accuracy: 0.2000\n",
      "INFO:tensorflow:Sat Nov 17 21:10:56 2018: Epoch: 007 Step/Batch: 000000356 Step mean time: 1434ms \n",
      "Loss: 2.5697999 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:11:06 2018: Epoch: 007 Step/Batch: 000000357 Step mean time: 1447ms \n",
      "Loss: 2.7267692 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:11:17 2018: Epoch: 007 Step/Batch: 000000358 Step mean time: 1461ms \n",
      "Loss: 2.6027627 Training accuracy: 0.1250\n",
      "INFO:tensorflow:Sat Nov 17 21:11:27 2018: Epoch: 007 Step/Batch: 000000359 Step mean time: 1474ms \n",
      "Loss: 2.7282333 Training accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Sat Nov 17 21:11:36 2018: Epoch: 007 Step/Batch: 000000360 Step mean time: 1487ms \n",
      "Loss: 2.6306515 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:11:48 2018: Elapsed time: 1097ms --- Loss: 2.6729255 Validation accuracy: 0.1800\n",
      "INFO:tensorflow:Sat Nov 17 21:11:49 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:11:59 2018: Epoch: 007 Step/Batch: 000000361 Step mean time: 1498ms \n",
      "Loss: 2.6812701 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:12:09 2018: Epoch: 007 Step/Batch: 000000362 Step mean time: 1511ms \n",
      "Loss: 2.6826246 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:12:19 2018: Epoch: 007 Step/Batch: 000000363 Step mean time: 1524ms \n",
      "Loss: 2.5294156 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:12:29 2018: Epoch: 007 Step/Batch: 000000364 Step mean time: 1536ms \n",
      "Loss: 2.4809437 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:12:34 2018: Epoch: 007 Step/Batch: 000000365 Step mean time: 1541ms \n",
      "Loss: 2.1858656 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:12:46 2018: Elapsed time: 1154ms --- Loss: 2.6010780 Validation accuracy: 0.2200\n",
      "INFO:tensorflow:Sat Nov 17 21:12:56 2018: Epoch: 008 Step/Batch: 000000366 Step mean time: 1552ms \n",
      "Loss: 2.4234302 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:13:05 2018: Epoch: 008 Step/Batch: 000000367 Step mean time: 1564ms \n",
      "Loss: 2.5275350 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:13:15 2018: Epoch: 008 Step/Batch: 000000368 Step mean time: 1577ms \n",
      "Loss: 2.6449585 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:13:25 2018: Epoch: 008 Step/Batch: 000000369 Step mean time: 1588ms \n",
      "Loss: 2.5975528 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:13:34 2018: Epoch: 008 Step/Batch: 000000370 Step mean time: 1599ms \n",
      "Loss: 2.5985541 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:13:46 2018: Elapsed time: 1215ms --- Loss: 2.5854876 Validation accuracy: 0.2400\n",
      "INFO:tensorflow:Sat Nov 17 21:13:47 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:13:57 2018: Epoch: 008 Step/Batch: 000000371 Step mean time: 1611ms \n",
      "Loss: 2.3248115 Training accuracy: 0.6250\n",
      "INFO:tensorflow:Sat Nov 17 21:14:07 2018: Epoch: 008 Step/Batch: 000000372 Step mean time: 1623ms \n",
      "Loss: 2.5252228 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:14:18 2018: Epoch: 008 Step/Batch: 000000373 Step mean time: 1636ms \n",
      "Loss: 2.3147349 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:14:29 2018: Epoch: 008 Step/Batch: 000000374 Step mean time: 1650ms \n",
      "Loss: 2.5697539 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:14:39 2018: Epoch: 008 Step/Batch: 000000375 Step mean time: 1662ms \n",
      "Loss: 2.4126184 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:14:52 2018: Elapsed time: 1280ms --- Loss: 2.6123013 Validation accuracy: 0.1600\n",
      "INFO:tensorflow:Sat Nov 17 21:15:01 2018: Epoch: 008 Step/Batch: 000000376 Step mean time: 1674ms \n",
      "Loss: 2.3915384 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:15:10 2018: Epoch: 008 Step/Batch: 000000377 Step mean time: 1684ms \n",
      "Loss: 2.4557643 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:15:15 2018: Epoch: 008 Step/Batch: 000000378 Step mean time: 1687ms \n",
      "Loss: 2.0897446 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:15:25 2018: Epoch: 009 Step/Batch: 000000379 Step mean time: 1698ms \n",
      "Loss: 2.0090458 Training accuracy: 0.5625\n",
      "INFO:tensorflow:Sat Nov 17 21:15:34 2018: Epoch: 009 Step/Batch: 000000380 Step mean time: 1709ms \n",
      "Loss: 2.5206776 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:15:46 2018: Elapsed time: 1334ms --- Loss: 2.6369557 Validation accuracy: 0.2000\n",
      "INFO:tensorflow:Sat Nov 17 21:15:46 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:15:55 2018: Epoch: 009 Step/Batch: 000000381 Step mean time: 1720ms \n",
      "Loss: 2.4888136 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:16:05 2018: Epoch: 009 Step/Batch: 000000382 Step mean time: 1731ms \n",
      "Loss: 2.3704691 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:16:14 2018: Epoch: 009 Step/Batch: 000000383 Step mean time: 1741ms \n",
      "Loss: 2.5161519 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:16:25 2018: Epoch: 009 Step/Batch: 000000384 Step mean time: 1753ms \n",
      "Loss: 2.4238858 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:16:35 2018: Epoch: 009 Step/Batch: 000000385 Step mean time: 1766ms \n",
      "Loss: 2.2550478 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:16:49 2018: Elapsed time: 1397ms --- Loss: 2.7355161 Validation accuracy: 0.1400\n",
      "INFO:tensorflow:Sat Nov 17 21:17:00 2018: Epoch: 009 Step/Batch: 000000386 Step mean time: 1780ms \n",
      "Loss: 2.2943954 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:17:11 2018: Epoch: 009 Step/Batch: 000000387 Step mean time: 1792ms \n",
      "Loss: 2.6236691 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:17:22 2018: Epoch: 009 Step/Batch: 000000388 Step mean time: 1805ms \n",
      "Loss: 2.2731264 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:17:32 2018: Epoch: 009 Step/Batch: 000000389 Step mean time: 1817ms \n",
      "Loss: 2.3441648 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:17:43 2018: Epoch: 009 Step/Batch: 000000390 Step mean time: 1828ms \n",
      "Loss: 2.2764261 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:17:57 2018: Elapsed time: 1465ms --- Loss: 2.6499560 Validation accuracy: 0.2000\n",
      "INFO:tensorflow:Sat Nov 17 21:17:57 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:18:02 2018: Epoch: 009 Step/Batch: 000000391 Step mean time: 1831ms \n",
      "Loss: 2.0134783 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:18:12 2018: Epoch: 010 Step/Batch: 000000392 Step mean time: 1842ms \n",
      "Loss: 1.9588180 Training accuracy: 0.5625\n",
      "INFO:tensorflow:Sat Nov 17 21:18:22 2018: Epoch: 010 Step/Batch: 000000393 Step mean time: 1852ms \n",
      "Loss: 2.5053673 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:18:32 2018: Epoch: 010 Step/Batch: 000000394 Step mean time: 1863ms \n",
      "Loss: 2.5447624 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:18:41 2018: Epoch: 010 Step/Batch: 000000395 Step mean time: 1873ms \n",
      "Loss: 2.5801420 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:18:54 2018: Elapsed time: 1522ms --- Loss: 2.6583102 Validation accuracy: 0.2200\n",
      "INFO:tensorflow:Sat Nov 17 21:19:04 2018: Epoch: 010 Step/Batch: 000000396 Step mean time: 1885ms \n",
      "Loss: 2.3538675 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:19:14 2018: Epoch: 010 Step/Batch: 000000397 Step mean time: 1896ms \n",
      "Loss: 2.3886530 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:19:25 2018: Epoch: 010 Step/Batch: 000000398 Step mean time: 1908ms \n",
      "Loss: 2.3664227 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:19:35 2018: Epoch: 010 Step/Batch: 000000399 Step mean time: 1918ms \n",
      "Loss: 2.1070266 Training accuracy: 0.5625\n",
      "INFO:tensorflow:Sat Nov 17 21:19:45 2018: Epoch: 010 Step/Batch: 000000400 Step mean time: 1930ms \n",
      "Loss: 2.2983708 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:19:59 2018: Elapsed time: 1587ms --- Loss: 2.6507678 Validation accuracy: 0.1600\n",
      "INFO:tensorflow:Sat Nov 17 21:19:59 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:20:10 2018: Epoch: 010 Step/Batch: 000000401 Step mean time: 1942ms \n",
      "Loss: 2.2071109 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:20:20 2018: Epoch: 010 Step/Batch: 000000402 Step mean time: 1953ms \n",
      "Loss: 2.3077776 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:20:31 2018: Epoch: 010 Step/Batch: 000000403 Step mean time: 1964ms \n",
      "Loss: 2.2710032 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:20:36 2018: Epoch: 010 Step/Batch: 000000404 Step mean time: 1967ms \n",
      "Loss: 1.9817028 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:20:47 2018: Epoch: 011 Step/Batch: 000000405 Step mean time: 1979ms \n",
      "Loss: 2.0256436 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:21:01 2018: Elapsed time: 1649ms --- Loss: 2.6811516 Validation accuracy: 0.1600\n",
      "INFO:tensorflow:Sat Nov 17 21:21:12 2018: Epoch: 011 Step/Batch: 000000406 Step mean time: 1990ms \n",
      "Loss: 2.4590268 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:21:22 2018: Epoch: 011 Step/Batch: 000000407 Step mean time: 2001ms \n",
      "Loss: 2.3352218 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:21:33 2018: Epoch: 011 Step/Batch: 000000408 Step mean time: 2013ms \n",
      "Loss: 2.1285746 Training accuracy: 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Sat Nov 17 21:21:43 2018: Epoch: 011 Step/Batch: 000000409 Step mean time: 2023ms \n",
      "Loss: 2.2248774 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:21:53 2018: Epoch: 011 Step/Batch: 000000410 Step mean time: 2034ms \n",
      "Loss: 2.2881181 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:22:06 2018: Elapsed time: 1714ms --- Loss: 2.6168349 Validation accuracy: 0.1600\n",
      "INFO:tensorflow:Sat Nov 17 21:22:07 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:22:17 2018: Epoch: 011 Step/Batch: 000000411 Step mean time: 2044ms \n",
      "Loss: 2.2404289 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:22:27 2018: Epoch: 011 Step/Batch: 000000412 Step mean time: 2054ms \n",
      "Loss: 1.9775455 Training accuracy: 0.5625\n",
      "INFO:tensorflow:Sat Nov 17 21:22:37 2018: Epoch: 011 Step/Batch: 000000413 Step mean time: 2065ms \n",
      "Loss: 2.2727094 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:22:47 2018: Epoch: 011 Step/Batch: 000000414 Step mean time: 2075ms \n",
      "Loss: 2.1147606 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:22:57 2018: Epoch: 011 Step/Batch: 000000415 Step mean time: 2085ms \n",
      "Loss: 2.1910195 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:23:10 2018: Elapsed time: 1778ms --- Loss: 2.6815732 Validation accuracy: 0.1400\n",
      "INFO:tensorflow:Sat Nov 17 21:23:20 2018: Epoch: 011 Step/Batch: 000000416 Step mean time: 2095ms \n",
      "Loss: 2.2348423 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:23:25 2018: Epoch: 011 Step/Batch: 000000417 Step mean time: 2098ms \n",
      "Loss: 1.7388277 Training accuracy: 0.7500\n",
      "INFO:tensorflow:Sat Nov 17 21:23:35 2018: Epoch: 012 Step/Batch: 000000418 Step mean time: 2107ms \n",
      "Loss: 1.9641922 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:23:45 2018: Epoch: 012 Step/Batch: 000000419 Step mean time: 2117ms \n",
      "Loss: 2.5013671 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:23:55 2018: Epoch: 012 Step/Batch: 000000420 Step mean time: 2126ms \n",
      "Loss: 2.2469797 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:24:07 2018: Elapsed time: 1836ms --- Loss: 2.8486266 Validation accuracy: 0.1800\n",
      "INFO:tensorflow:Sat Nov 17 21:24:08 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:24:18 2018: Epoch: 012 Step/Batch: 000000421 Step mean time: 2136ms \n",
      "Loss: 2.0231633 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:24:28 2018: Epoch: 012 Step/Batch: 000000422 Step mean time: 2146ms \n",
      "Loss: 2.3205614 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:24:39 2018: Epoch: 012 Step/Batch: 000000423 Step mean time: 2156ms \n",
      "Loss: 2.2549152 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:24:49 2018: Epoch: 012 Step/Batch: 000000424 Step mean time: 2166ms \n",
      "Loss: 2.0591679 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:24:59 2018: Epoch: 012 Step/Batch: 000000425 Step mean time: 2176ms \n",
      "Loss: 2.1238556 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:25:11 2018: Elapsed time: 1900ms --- Loss: 2.6697254 Validation accuracy: 0.2200\n",
      "INFO:tensorflow:Sat Nov 17 21:25:22 2018: Epoch: 012 Step/Batch: 000000426 Step mean time: 2185ms \n",
      "Loss: 2.2789459 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:25:32 2018: Epoch: 012 Step/Batch: 000000427 Step mean time: 2195ms \n",
      "Loss: 1.9404402 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:25:42 2018: Epoch: 012 Step/Batch: 000000428 Step mean time: 2205ms \n",
      "Loss: 2.2264557 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:25:52 2018: Epoch: 012 Step/Batch: 000000429 Step mean time: 2214ms \n",
      "Loss: 2.2250011 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:25:57 2018: Epoch: 012 Step/Batch: 000000430 Step mean time: 2216ms \n",
      "Loss: 1.8988261 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:26:09 2018: Elapsed time: 1957ms --- Loss: 2.6093974 Validation accuracy: 0.2200\n",
      "INFO:tensorflow:Sat Nov 17 21:26:10 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:26:20 2018: Epoch: 013 Step/Batch: 000000431 Step mean time: 2225ms \n",
      "Loss: 1.9008627 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:26:32 2018: Epoch: 013 Step/Batch: 000000432 Step mean time: 2236ms \n",
      "Loss: 2.3973420 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:26:43 2018: Epoch: 013 Step/Batch: 000000433 Step mean time: 2247ms \n",
      "Loss: 2.6789367 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:26:54 2018: Epoch: 013 Step/Batch: 000000434 Step mean time: 2258ms \n",
      "Loss: 2.4251184 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:27:04 2018: Epoch: 013 Step/Batch: 000000435 Step mean time: 2267ms \n",
      "Loss: 2.4102979 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:27:15 2018: Elapsed time: 2024ms --- Loss: 2.5534048 Validation accuracy: 0.2600\n",
      "INFO:tensorflow:Sat Nov 17 21:27:26 2018: Epoch: 013 Step/Batch: 000000436 Step mean time: 2276ms \n",
      "Loss: 2.2494516 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:27:36 2018: Epoch: 013 Step/Batch: 000000437 Step mean time: 2284ms \n",
      "Loss: 2.2878580 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:27:47 2018: Epoch: 013 Step/Batch: 000000438 Step mean time: 2294ms \n",
      "Loss: 2.0822289 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:27:56 2018: Epoch: 013 Step/Batch: 000000439 Step mean time: 2303ms \n",
      "Loss: 2.2468395 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:28:06 2018: Epoch: 013 Step/Batch: 000000440 Step mean time: 2311ms \n",
      "Loss: 1.8677120 Training accuracy: 0.5625\n",
      "INFO:tensorflow:Sat Nov 17 21:28:18 2018: Elapsed time: 2087ms --- Loss: 2.5640917 Validation accuracy: 0.2600\n",
      "INFO:tensorflow:Sat Nov 17 21:28:19 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:28:29 2018: Epoch: 013 Step/Batch: 000000441 Step mean time: 2321ms \n",
      "Loss: 2.4085357 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:28:39 2018: Epoch: 013 Step/Batch: 000000442 Step mean time: 2330ms \n",
      "Loss: 2.1575649 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:28:45 2018: Epoch: 013 Step/Batch: 000000443 Step mean time: 2332ms \n",
      "Loss: 1.9162569 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:28:55 2018: Epoch: 014 Step/Batch: 000000444 Step mean time: 2341ms \n",
      "Loss: 1.7794368 Training accuracy: 0.6250\n",
      "INFO:tensorflow:Sat Nov 17 21:29:07 2018: Epoch: 014 Step/Batch: 000000445 Step mean time: 2351ms \n",
      "Loss: 2.2558305 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:29:19 2018: Elapsed time: 2147ms --- Loss: 2.4869194 Validation accuracy: 0.2600\n",
      "INFO:tensorflow:Sat Nov 17 21:29:29 2018: Epoch: 014 Step/Batch: 000000446 Step mean time: 2359ms \n",
      "Loss: 2.2110238 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:29:39 2018: Epoch: 014 Step/Batch: 000000447 Step mean time: 2367ms \n",
      "Loss: 2.1078441 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:29:48 2018: Epoch: 014 Step/Batch: 000000448 Step mean time: 2376ms \n",
      "Loss: 2.2648733 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:29:58 2018: Epoch: 014 Step/Batch: 000000449 Step mean time: 2384ms \n",
      "Loss: 2.2716432 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:30:08 2018: Epoch: 014 Step/Batch: 000000450 Step mean time: 2392ms \n",
      "Loss: 2.1008275 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:30:21 2018: Elapsed time: 2209ms --- Loss: 2.5988972 Validation accuracy: 0.2200\n",
      "INFO:tensorflow:Sat Nov 17 21:30:21 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:30:31 2018: Epoch: 014 Step/Batch: 000000451 Step mean time: 2401ms \n",
      "Loss: 1.7092118 Training accuracy: 0.6250\n",
      "INFO:tensorflow:Sat Nov 17 21:30:41 2018: Epoch: 014 Step/Batch: 000000452 Step mean time: 2409ms \n",
      "Loss: 2.3642058 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:30:51 2018: Epoch: 014 Step/Batch: 000000453 Step mean time: 2417ms \n",
      "Loss: 2.0973914 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:31:01 2018: Epoch: 014 Step/Batch: 000000454 Step mean time: 2425ms \n",
      "Loss: 2.2321105 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:31:11 2018: Epoch: 014 Step/Batch: 000000455 Step mean time: 2433ms \n",
      "Loss: 2.7988310 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:31:23 2018: Elapsed time: 2271ms --- Loss: 2.6171827 Validation accuracy: 0.2400\n",
      "INFO:tensorflow:Sat Nov 17 21:31:28 2018: Epoch: 014 Step/Batch: 000000456 Step mean time: 2436ms \n",
      "Loss: 1.9452152 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:31:39 2018: Epoch: 015 Step/Batch: 000000457 Step mean time: 2446ms \n",
      "Loss: 2.4263411 Training accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Sat Nov 17 21:31:50 2018: Epoch: 015 Step/Batch: 000000458 Step mean time: 2455ms \n",
      "Loss: 2.3939660 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:32:00 2018: Epoch: 015 Step/Batch: 000000459 Step mean time: 2463ms \n",
      "Loss: 2.7778006 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:32:10 2018: Epoch: 015 Step/Batch: 000000460 Step mean time: 2471ms \n",
      "Loss: 2.2262745 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:32:23 2018: Elapsed time: 2331ms --- Loss: 2.4606037 Validation accuracy: 0.2200\n",
      "INFO:tensorflow:Sat Nov 17 21:32:23 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:32:33 2018: Epoch: 015 Step/Batch: 000000461 Step mean time: 2479ms \n",
      "Loss: 2.5257080 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:32:43 2018: Epoch: 015 Step/Batch: 000000462 Step mean time: 2487ms \n",
      "Loss: 2.3301249 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:32:54 2018: Epoch: 015 Step/Batch: 000000463 Step mean time: 2496ms \n",
      "Loss: 2.1908667 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:33:04 2018: Epoch: 015 Step/Batch: 000000464 Step mean time: 2503ms \n",
      "Loss: 2.0975931 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:33:14 2018: Epoch: 015 Step/Batch: 000000465 Step mean time: 2511ms \n",
      "Loss: 2.4550405 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:33:27 2018: Elapsed time: 2395ms --- Loss: 2.4874902 Validation accuracy: 0.2600\n",
      "INFO:tensorflow:Sat Nov 17 21:33:37 2018: Epoch: 015 Step/Batch: 000000466 Step mean time: 2520ms \n",
      "Loss: 2.1597409 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:33:47 2018: Epoch: 015 Step/Batch: 000000467 Step mean time: 2528ms \n",
      "Loss: 2.1510329 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:33:57 2018: Epoch: 015 Step/Batch: 000000468 Step mean time: 2536ms \n",
      "Loss: 2.3654590 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:34:02 2018: Epoch: 015 Step/Batch: 000000469 Step mean time: 2537ms \n",
      "Loss: 1.9172118 Training accuracy: 0.6250\n",
      "INFO:tensorflow:Sat Nov 17 21:34:13 2018: Epoch: 016 Step/Batch: 000000470 Step mean time: 2546ms \n",
      "Loss: 2.1556692 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:34:27 2018: Elapsed time: 2455ms --- Loss: 2.6881855 Validation accuracy: 0.2800\n",
      "INFO:tensorflow:Sat Nov 17 21:34:28 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:34:38 2018: Epoch: 016 Step/Batch: 000000471 Step mean time: 2555ms \n",
      "Loss: 2.3694074 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:34:48 2018: Epoch: 016 Step/Batch: 000000472 Step mean time: 2562ms \n",
      "Loss: 2.6539617 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:34:58 2018: Epoch: 016 Step/Batch: 000000473 Step mean time: 2570ms \n",
      "Loss: 2.5822208 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:35:08 2018: Epoch: 016 Step/Batch: 000000474 Step mean time: 2577ms \n",
      "Loss: 2.4049871 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:35:18 2018: Epoch: 016 Step/Batch: 000000475 Step mean time: 2585ms \n",
      "Loss: 2.3462543 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:35:32 2018: Elapsed time: 2520ms --- Loss: 2.4281464 Validation accuracy: 0.3000\n",
      "INFO:tensorflow:Sat Nov 17 21:35:43 2018: Epoch: 016 Step/Batch: 000000476 Step mean time: 2594ms \n",
      "Loss: 2.4713042 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:35:53 2018: Epoch: 016 Step/Batch: 000000477 Step mean time: 2601ms \n",
      "Loss: 2.2436230 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:36:04 2018: Epoch: 016 Step/Batch: 000000478 Step mean time: 2610ms \n",
      "Loss: 2.4550414 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:36:16 2018: Epoch: 016 Step/Batch: 000000479 Step mean time: 2620ms \n",
      "Loss: 2.4826369 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:36:26 2018: Epoch: 016 Step/Batch: 000000480 Step mean time: 2627ms \n",
      "Loss: 2.5404341 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:36:39 2018: Elapsed time: 2587ms --- Loss: 2.3933456 Validation accuracy: 0.2600\n",
      "INFO:tensorflow:Sat Nov 17 21:36:39 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:36:50 2018: Epoch: 016 Step/Batch: 000000481 Step mean time: 2636ms \n",
      "Loss: 2.4295788 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:36:55 2018: Epoch: 016 Step/Batch: 000000482 Step mean time: 2637ms \n",
      "Loss: 1.9802159 Training accuracy: 0.7500\n",
      "INFO:tensorflow:Sat Nov 17 21:37:06 2018: Epoch: 017 Step/Batch: 000000483 Step mean time: 2645ms \n",
      "Loss: 2.0887041 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:37:16 2018: Epoch: 017 Step/Batch: 000000484 Step mean time: 2653ms \n",
      "Loss: 2.5159287 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:37:26 2018: Epoch: 017 Step/Batch: 000000485 Step mean time: 2660ms \n",
      "Loss: 2.7971220 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:37:39 2018: Elapsed time: 2647ms --- Loss: 2.3708062 Validation accuracy: 0.2600\n",
      "INFO:tensorflow:Sat Nov 17 21:37:49 2018: Epoch: 017 Step/Batch: 000000486 Step mean time: 2668ms \n",
      "Loss: 2.5016534 Training accuracy: 0.1875\n",
      "INFO:tensorflow:Sat Nov 17 21:37:59 2018: Epoch: 017 Step/Batch: 000000487 Step mean time: 2675ms \n",
      "Loss: 2.3346343 Training accuracy: 0.3125\n",
      "INFO:tensorflow:Sat Nov 17 21:38:10 2018: Epoch: 017 Step/Batch: 000000488 Step mean time: 2683ms \n",
      "Loss: 2.2306707 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:38:20 2018: Epoch: 017 Step/Batch: 000000489 Step mean time: 2690ms \n",
      "Loss: 2.3078058 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:38:30 2018: Epoch: 017 Step/Batch: 000000490 Step mean time: 2697ms \n",
      "Loss: 2.0933857 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:38:42 2018: Elapsed time: 2711ms --- Loss: 2.2675958 Validation accuracy: 0.2400\n",
      "INFO:tensorflow:Sat Nov 17 21:38:43 2018: Variables saved\n",
      "INFO:tensorflow:Sat Nov 17 21:38:53 2018: Epoch: 017 Step/Batch: 000000491 Step mean time: 2704ms \n",
      "Loss: 2.5139031 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:39:03 2018: Epoch: 017 Step/Batch: 000000492 Step mean time: 2711ms \n",
      "Loss: 2.4361594 Training accuracy: 0.4375\n",
      "INFO:tensorflow:Sat Nov 17 21:39:14 2018: Epoch: 017 Step/Batch: 000000493 Step mean time: 2720ms \n",
      "Loss: 2.3739696 Training accuracy: 0.3750\n",
      "INFO:tensorflow:Sat Nov 17 21:39:25 2018: Epoch: 017 Step/Batch: 000000494 Step mean time: 2728ms \n",
      "Loss: 2.5659275 Training accuracy: 0.2500\n",
      "INFO:tensorflow:Sat Nov 17 21:39:31 2018: Epoch: 017 Step/Batch: 000000495 Step mean time: 2730ms \n",
      "Loss: 1.8709345 Training accuracy: 0.5000\n",
      "INFO:tensorflow:Sat Nov 17 21:39:44 2018: Elapsed time: 2772ms --- Loss: 2.3228879 Validation accuracy: 0.2800\n",
      "INFO:tensorflow:Sat Nov 17 21:39:55 2018: Epoch: 018 Step/Batch: 000000496 Step mean time: 2739ms \n",
      "Loss: 2.0457964 Training accuracy: 0.3125\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(CKPT_PATH):\n",
    "    os.makedirs(CKPT_PATH)\n",
    "train(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (_, (test_images, test_labels)) in enumerate(test_dset):\n",
    "    l = loss(alexnet, 'val', test_images, test_labels)\n",
    "    a = accuracy(alexnet, 'val', test_images, test_labels).numpy()\n",
    "log_msg('Final Test Loss: {:.7f} Test accuracy: {:.4f}'.format(l, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
